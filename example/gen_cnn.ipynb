{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7057c08",
   "metadata": {},
   "source": [
    "<h1>CNNs for EPRV</h1>\n",
    "The goal here is to training a CNN using HARPS images to the outputs of the HARPS EPRV extraction pipeline to see it a large of NN can replicated more explicit modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c71f8e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf1909b",
   "metadata": {},
   "source": [
    "<h2>Model definition</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "332ae65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSizeNet(nn.Module):\n",
    "    def __init__(self, in_size, out_size, normalize=True, leaky_slope=0.2):\n",
    "        super(DownSizeNet, self).__init__()\n",
    "        layers = [nn.Conv2d(in_size, out_size, 3, stride=2, padding=1, bias=False).double()]\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_size, 0.8).double())\n",
    "        layers.append(nn.LeakyReLU(leaky_slope).double())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class RV_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RV_Model, self).__init__()\n",
    "#         channels_in, self.h, self.w = in_shape\n",
    "#         channels_out, _, _ = out_shape\n",
    "\n",
    "#         self.fc    = nn.Linear(latent_dim, self.h * self.w)\n",
    "\n",
    "        self.down1 = DownSizeNet(3, 64, normalize=False)\n",
    "        self.down2 = DownSizeNet(64, 128)\n",
    "        self.down3 = DownSizeNet(128, 256)\n",
    "        self.down4 = DownSizeNet(256, 512)\n",
    "        self.down5 = DownSizeNet(512, 512)\n",
    "        self.down6 = DownSizeNet(512, 512)\n",
    "#         self.down7 = DownSizeNet(512, 1, normalize=False)\n",
    "        \n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, 3, stride=1, padding=1).double(), nn.Tanh().double()\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Propogate noise through fc layer and reshape to img shape\n",
    "#         z = self.fc(z).view(z.size(0), 1, self.h, self.w)\n",
    "        d1 = self.down1(x)\n",
    "#         print('d1: {}'.format(d1.shape))\n",
    "        d2 = self.down2(d1)\n",
    "#         print('d2: {}'.format(d2.shape))\n",
    "        d3 = self.down3(d2)\n",
    "#         print('d3: {}'.format(d3.shape))\n",
    "        d4 = self.down4(d3)\n",
    "#         print('d4: {}'.format(d4.shape))\n",
    "        d5 = self.down5(d4)\n",
    "#         print('d5: {}'.format(d5.shape))\n",
    "        d6 = self.down6(d5)\n",
    "#         print('d6: {}'.format(d6.shape))\n",
    "#         d7 = self.down7(d6)\n",
    "#         print('d7: {}'.format(d7.shape))\n",
    "        \n",
    "        \n",
    "        return self.final(d6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "020d3e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "692c9f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e409343",
   "metadata": {},
   "source": [
    "<h2>Dataset importing</h2>\n",
    "The question here is how is the data organized in the directory and how can it be imported with the target RV. \n",
    "\n",
    "Not the OG data but after the data is saved from the pre processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "987c8771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15a789a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "      with open(filename, 'rb') as handle:\n",
    "            obj = pickle.load(handle)\n",
    "            return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1605c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_train = load_data('../data/xs_train.pickle')\n",
    "ys_train = load_data('../data/ys_train.pickle')\n",
    "xs_valid = load_data('../data/xs_valid.pickle')\n",
    "ys_valid = load_data('../data/ys_valid.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e588fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RV_Dataset(Dataset):\n",
    "    def __init__(self, xs_data, ys_data):\n",
    "       \n",
    "        # NO RESIZE\n",
    "        self.transform = transforms.ToTensor()#transforms.Compose(\n",
    "\n",
    "        print(self.transform,type(self.transform))\n",
    "        self.data = {'x':xs_data,'y':ys_data}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.transform(self.data['x'][index]).to(torch.double), self.data['y'][index]\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data['y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fe6589fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.nddata import Cutout2D\n",
    "from astropy import units as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a57b804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7130b20e",
   "metadata": {},
   "source": [
    "<h2>Preprocess</h2>\n",
    "take in the FITS images and chop them up to sizes small enough to fit on a GPU with the model. Then these files are fed into the Dataset above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3c56c0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_names,outdir,size):\n",
    "    files = sorted(glob.glob(file_names))\n",
    "    for file_name in files:\n",
    "        tbl = fits.open(file_name)\n",
    "        img_shape = tbl[0].data.shape\n",
    "        integer = (img_shape[1]//size) + 1\n",
    "        delta_x = (img_shape[1] - size)/integer\n",
    "        for i in range(integer):\n",
    "            position = (img_shape[0]/2, size/2 + i * delta_x)\n",
    "            size     = (shape[0], size)\n",
    "            sci   = Cutout2D(tbl['sci'], position, size)\n",
    "            ref   = Cutout2D(tbl['ref'], position, size)\n",
    "            \n",
    "            sci_hdu = fits.ImageHDU(data=sci)\n",
    "            ref_hdu = fits.ImageHDU(data=ref)\n",
    "            \n",
    "            hdu = fits.HDUList([sci_hdu,ref_hdu])\n",
    "            name = path.join(outdir,path.split(file_name)[1] + '_c{}.fits'.format(i))\n",
    "            hdu.header['RV'] = tbl['RV']\n",
    "            hdu.writeto(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb8a6cf",
   "metadata": {},
   "source": [
    "<h2>Defining Fitting Process</h2>\n",
    "including hyperparameters, the loss function, and the optimization algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "aa2b7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001#, betas=(, ),\n",
    "b1 = 0.9\n",
    "b2 = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0cfd1cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RV_Model()\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fbb9a477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 128, 512, 3), (100,))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs_train.shape, ys_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "cf2cad0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToTensor() <class 'torchvision.transforms.transforms.ToTensor'>\n",
      "ToTensor() <class 'torchvision.transforms.transforms.ToTensor'>\n"
     ]
    }
   ],
   "source": [
    "# directory = something\n",
    "batch_size = 2\n",
    "n_cpu = 1\n",
    "dataloader = DataLoader(\n",
    "    RV_Dataset(xs_train,ys_train),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=n_cpu,\n",
    ")\n",
    "\n",
    "validloader = DataLoader(\n",
    "    RV_Dataset(xs_valid,ys_valid),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=n_cpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8739cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920fe1b3",
   "metadata": {},
   "source": [
    "<h2>Training Step</h2>\n",
    "the working step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ae999c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "991ff708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/1] [Batch 45/50] [Loss: 381.597336]"
     ]
    }
   ],
   "source": [
    "prev_time = time.time()\n",
    "n_epochs = 1\n",
    "validiter = iter(validloader)\n",
    "for epoch in range(n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        def into_func():\n",
    "            imgs, target_rv = batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "#             print('imgs: {}'.format(imgs.shape))\n",
    "            something = model(imgs)\n",
    "#             print('something: {}'.format(something.shape))\n",
    "            y = torch.mean(something)\n",
    "#             print('y: {} {}'.format(y.shape,y.dtype))\n",
    "#             print('target_rv: {} {}'.format(target_rv.shape,target_rv.dtype))\n",
    "            loss = mse_loss(y,target_rv.double())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "#             print('step {}'.format(i))\n",
    "\n",
    "#             batches_done = epoch * len(dataloader) + i\n",
    "#             batches_left = n_epochs * len(dataloader) - batches_done\n",
    "#             time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "#             prev_time = time.time()\n",
    "\n",
    "            # Print log\n",
    "            \n",
    "        into_func()\n",
    "        if i % 5 == 0:\n",
    "            imgs, target_rv = validiter.next()\n",
    "            optimizer.zero_grad()\n",
    "            something = model(imgs)\n",
    "            y = torch.mean(something)\n",
    "            loss = mse_loss(y,target_rv.double())\n",
    "            \n",
    "            sys.stdout.write(\n",
    "                \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f]\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    n_epochs,\n",
    "                    i,\n",
    "                    len(dataloader),\n",
    "                    loss.item()\n",
    "                )  \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9110e417",
   "metadata": {},
   "source": [
    "<h2>Slicing whole image problem</h2>\n",
    "Since a whole HARPS image is 4096 pixels, 8 512x512 images could be used to exactly divide the whole. But since there is information in the over lap of any two cross sections within pixel distance less than the assiocated max velocity. Or just more than the previous known velocity precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c4473d",
   "metadata": {},
   "source": [
    "c-m-c-m-c-m-c there are m divisions of image of whole size (c+m+c+m+c+m+c=3m+4c)^2 into boxes of size (c+m+c=2c+m)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ec104a",
   "metadata": {},
   "source": [
    "so if m = 512, and c = 6, then 8 divisions of size 524x524, which is a total image size of 8(512-72x/8)+9(0+72x/9)=4096 = > \n",
    "8(512-9)+9(8) = 8(503) + 9(8)=4096\n",
    "\n",
    "This first integer value is good because we can avoid interpolation, and we can rest assured that no predicted measurements exceed 8 pixels to log-wavelength\n",
    "\n",
    "it = np.zeros(8,2)\n",
    "it[0,0] = 0\n",
    "it[0,1] = 2c+m = 2(8) + 503 = 16 + 503 = 519\n",
    "\n",
    "it[i,:]=it[i-1,:]+c+m=it[i-1,:]+8+503=it[i-1,:]+511\n",
    "so to iterate through this list we select, it_{i+1}= it_{i} + 2c+m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1e39b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3eb10d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=503\n",
    "c=8\n",
    "itlist =  np.zeros((c,2)) \n",
    "itlist[0,0] = 0 \n",
    "itlist[0,1] = 519\n",
    "for i in range(1,c):\n",
    "    itlist[i,:]=itlist[i-1,:]+511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "54fc3697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.  519.]\n",
      " [ 511. 1030.]\n",
      " [1022. 1541.]\n",
      " [1533. 2052.]\n",
      " [2044. 2563.]\n",
      " [2555. 3074.]\n",
      " [3066. 3585.]\n",
      " [3577. 4096.]]\n"
     ]
    }
   ],
   "source": [
    "print(itlist)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b950abe",
   "metadata": {},
   "source": [
    "or for 8(512-18)+9(0+16)=8(494)+9(16)=8(512)-8(18)+9(16)=8(512)-16(9)+9(16)=8(512)=4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8266857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=494\n",
    "c=16\n",
    "n=8\n",
    "itlist =  np.zeros((n,2)) \n",
    "itlist[0,0] = 0 \n",
    "itlist[0,1] = m+2*c\n",
    "for i in range(1,n):\n",
    "    itlist[i,:]=itlist[i-1,:]+m+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "35eb0963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.  526.]\n",
      " [ 510. 1036.]\n",
      " [1020. 1546.]\n",
      " [1530. 2056.]\n",
      " [2040. 2566.]\n",
      " [2550. 3076.]\n",
      " [3060. 3586.]\n",
      " [3570. 4096.]]\n"
     ]
    }
   ],
   "source": [
    "print(itlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7b22ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
